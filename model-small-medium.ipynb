{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad6f50dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99686069",
   "metadata": {},
   "source": [
    "**Data Preparation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a537084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "train_dataset = load_dataset(\"google-research-datasets/mbpp\", split=\"train\")\n",
    "eval_dataset = load_dataset(\"openai/openai_humaneval\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fadfa444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['task_id', 'text', 'code', 'test_list', 'test_setup_code', 'challenge_test_list'],\n",
      "    num_rows: 374\n",
      "})\n",
      "{\n",
      "  \"task_id\": 601,\n",
      "  \"text\": \"Write a function to find the longest chain which can be formed from the given set of pairs.\",\n",
      "  \"code\": \"class Pair(object): \\r\\n\\tdef __init__(self, a, b): \\r\\n\\t\\tself.a = a \\r\\n\\t\\tself.b = b \\r\\ndef max_chain_length(arr, n): \\r\\n\\tmax = 0\\r\\n\\tmcl = [1 for i in range(n)] \\r\\n\\tfor i in range(1, n): \\r\\n\\t\\tfor j in range(0, i): \\r\\n\\t\\t\\tif (arr[i].a > arr[j].b and\\r\\n\\t\\t\\t\\tmcl[i] < mcl[j] + 1): \\r\\n\\t\\t\\t\\tmcl[i] = mcl[j] + 1\\r\\n\\tfor i in range(n): \\r\\n\\t\\tif (max < mcl[i]): \\r\\n\\t\\t\\tmax = mcl[i] \\r\\n\\treturn max\",\n",
      "  \"test_list\": [\n",
      "    \"assert max_chain_length([Pair(5, 24), Pair(15, 25),Pair(27, 40), Pair(50, 60)], 4) == 3\",\n",
      "    \"assert max_chain_length([Pair(1, 2), Pair(3, 4),Pair(5, 6), Pair(7, 8)], 4) == 4\",\n",
      "    \"assert max_chain_length([Pair(19, 10), Pair(11, 12),Pair(13, 14), Pair(15, 16), Pair(31, 54)], 5) == 5\"\n",
      "  ],\n",
      "  \"test_setup_code\": \"\",\n",
      "  \"challenge_test_list\": []\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset)\n",
    "print(json.dumps((train_dataset[0]), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72c3f071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['task_id', 'prompt', 'canonical_solution', 'test', 'entry_point'],\n",
      "    num_rows: 164\n",
      "})\n",
      "{\n",
      "  \"task_id\": \"HumanEval/0\",\n",
      "  \"prompt\": \"from typing import List\\n\\n\\ndef has_close_elements(numbers: List[float], threshold: float) -> bool:\\n    \\\"\\\"\\\" Check if in given list of numbers, are any two numbers closer to each other than\\n    given threshold.\\n    >>> has_close_elements([1.0, 2.0, 3.0], 0.5)\\n    False\\n    >>> has_close_elements([1.0, 2.8, 3.0, 4.0, 5.0, 2.0], 0.3)\\n    True\\n    \\\"\\\"\\\"\\n\",\n",
      "  \"canonical_solution\": \"    for idx, elem in enumerate(numbers):\\n        for idx2, elem2 in enumerate(numbers):\\n            if idx != idx2:\\n                distance = abs(elem - elem2)\\n                if distance < threshold:\\n                    return True\\n\\n    return False\\n\",\n",
      "  \"test\": \"\\n\\nMETADATA = {\\n    'author': 'jt',\\n    'dataset': 'test'\\n}\\n\\n\\ndef check(candidate):\\n    assert candidate([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.3) == True\\n    assert candidate([1.0, 2.0, 3.9, 4.0, 5.0, 2.2], 0.05) == False\\n    assert candidate([1.0, 2.0, 5.9, 4.0, 5.0], 0.95) == True\\n    assert candidate([1.0, 2.0, 5.9, 4.0, 5.0], 0.8) == False\\n    assert candidate([1.0, 2.0, 3.0, 4.0, 5.0, 2.0], 0.1) == True\\n    assert candidate([1.1, 2.2, 3.1, 4.1, 5.1], 1.0) == True\\n    assert candidate([1.1, 2.2, 3.1, 4.1, 5.1], 0.5) == False\\n\\n\",\n",
      "  \"entry_point\": \"has_close_elements\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(eval_dataset)\n",
    "print(json.dumps((eval_dataset[0]), indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3aeebb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepocess_training_data_function(examples, tokenizer):\n",
    "    # Concatenate problem and solution for training examples\n",
    "    inputs = [f\"Instruction:\\n{problem}\\n\\nTest list:\\n{test}\\n\\nResponse:\\n\" for problem, test in zip(examples[\"text\"], examples[\"test_list\"])]\n",
    "    targets = examples[\"code\"]\n",
    "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True, padding=\"max_length\")\n",
    "    labels = tokenizer(targets, max_length=512, truncation=True, padding=\"max_length\").input_ids\n",
    "    model_inputs[\"labels\"] = labels\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47f2ac3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepocess_evaluation_data_function(examples, tokenizer):\n",
    "    # Concatenate problem and solution for training examples\n",
    "    inputs = [f\"Instruction:\\n{problem}\\n\\nTest list:\\n{test}\\n\\nResponse:\\n\" for problem, test in zip(examples[\"prompt\"], examples[\"test\"])]\n",
    "    targets = examples[\"canonical_solution\"]\n",
    "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True, padding=\"max_length\")\n",
    "    labels = tokenizer(targets, max_length=512, truncation=True, padding=\"max_length\").input_ids\n",
    "    model_inputs[\"labels\"] = labels\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2bc0d92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use tokenizer from teacher model for consistency\n",
    "checkpoint = \"bigcode/starcoder2-3b\" \n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint, use_fast=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8edb84bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa46c480467f449d95e2bc96721ffb62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/374 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_train = train_dataset.map(lambda x: prepocess_training_data_function(x, tokenizer), batched=True, remove_columns=train_dataset.column_names)\n",
    "tokenized_eval = eval_dataset.map(lambda x: prepocess_evaluation_data_function(x, tokenizer), batched=True, remove_columns=eval_dataset.column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a83ca51",
   "metadata": {},
   "source": [
    "**Build the teacher model**\n",
    "\n",
    "Load the teacher as a large pre-trained model and use it to generate soft labels for distillation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea51c86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "105b8590b7be4ef9970e5d24df63c998",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/12.1G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "teacher_model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\", dtype=\"auto\")\n",
    "teacher_model.eval()  # Set to evaluation mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32af30b",
   "metadata": {},
   "source": [
    "**Build the Student model from scratch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e215cca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
